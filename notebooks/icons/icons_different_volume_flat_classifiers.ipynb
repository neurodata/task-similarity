{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97dc0964",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib\n",
    "import torchvision\n",
    "import torch\n",
    "import os\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, plot_roc_curve, auc, accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "import gc\n",
    "import pickle\n",
    "import torch.utils.data as data_utils\n",
    "from torch.utils.data import DataLoader\n",
    "from tasksim import *\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import time\n",
    "\n",
    "\n",
    "matplotlib.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67aedbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IconDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, transform, data_root):\n",
    "        self.class_names = df['class'].unique().tolist()\n",
    "        self.image_dir = data_root\n",
    "        self.transform = transform\n",
    "        self.total = len(df)\n",
    "        self.image_names = df['image_path'].to_list()\n",
    "        self.labels = df['label'].to_numpy()\n",
    "        self.targets = self.labels\n",
    "                    \n",
    "    def __len__(self):\n",
    "        return self.total\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_name = self.image_names[idx]\n",
    "        image_path = os.path.join(self.image_dir, image_name)\n",
    "        image = self.transform(Image.open(image_path).convert('RGB'))\n",
    "        label = self.labels[idx]\n",
    "        target = self.labels[idx]\n",
    "        return image, label\n",
    "\n",
    "    \n",
    "image_size = (128, 128)\n",
    "resnet_mean = [0.485, 0.456, 0.406]\n",
    "resnet_std = [0.229, 0.224, 0.225]\n",
    "\n",
    "#Creating a Transformation Object\n",
    "train_transform = torchvision.transforms.Compose([\n",
    "    #Converting images to the size that the model expects\n",
    "    torchvision.transforms.Resize(size=image_size),\n",
    "    torchvision.transforms.RandomHorizontalFlip(), #A RandomHorizontalFlip to augment our data\n",
    "    torchvision.transforms.ToTensor(), #Converting to tensor\n",
    "    #Normalizing the data to the data that the ResNet18 was trained on\n",
    "    torchvision.transforms.Normalize(mean = resnet_mean ,\n",
    "                                    std = resnet_std) \n",
    "    \n",
    "])\n",
    "\n",
    "\n",
    "#Creating a Transformation Object\n",
    "test_transform = torchvision.transforms.Compose([\n",
    "    #Converting images to the size that the model expects\n",
    "    torchvision.transforms.Resize(size=image_size),\n",
    "    # We don't do data augmentation in the test/val set    \n",
    "    torchvision.transforms.ToTensor(), #Converting to tensor\n",
    "    torchvision.transforms.Normalize(mean = resnet_mean,\n",
    "                                    std = resnet_std) \n",
    "    \n",
    "])\n",
    "\n",
    "#build model using Resnext50 as backbone\n",
    "class Resnext50(torch.nn.Module):\n",
    "    def __init__(self, n_classes, name):\n",
    "        super().__init__()\n",
    "        self.name = name\n",
    "        resnet = torchvision.models.resnext50_32x4d(pretrained=True, progress=True)\n",
    "        resnet.fc = torch.nn.Sequential(\n",
    "            torch.nn.Dropout(p=0.2),\n",
    "            torch.nn.Linear(in_features=resnet.fc.in_features, out_features=n_classes)\n",
    "        )\n",
    "        self.base_model = resnet\n",
    "        self.soft = torch.nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.soft(self.base_model(x))\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer, mode=\"flat\", lookup=[], fine_to_coarse={}, \n",
    "               batch_size=64):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    counter = 0\n",
    "    with tqdm(dataloader, unit=\"batch\") as tepoch:\n",
    "        for X_cpu, y_cpu in tepoch:\n",
    "            counter += 1\n",
    "            if mode == \"fine\":\n",
    "                y_cpu = torch.tensor([lookup.index(i) for i in y_cpu])\n",
    "            elif mode == \"coarse\":\n",
    "                y_cpu = torch.tensor([fine_to_coarse[int(target)] for target in y_cpu])\n",
    "            X, y = X_cpu.to(device), y_cpu.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X)\n",
    "            loss = loss_fn(outputs, y.type(torch.long))\n",
    "            loss.backward() \n",
    "            optimizer.step() \n",
    "            train_loss += loss.item() \n",
    "            avg_loss = train_loss / counter\n",
    "            tepoch.set_postfix(loss=avg_loss)\n",
    "    return avg_loss\n",
    "    \n",
    "    \n",
    "def eval_model(dataloader, model, loss_fn, mode=\"flat\", lookup=[], fine_to_coarse={}, \n",
    "               return_probab=False, batch_size=64):\n",
    "    model.eval()\n",
    "    predicts = []\n",
    "    targets = []\n",
    "    val_running_loss = 0.0\n",
    "    \n",
    "    for counter, (images, labels) in enumerate(dataloader):\n",
    "        if mode == \"fine\":\n",
    "            labels = torch.tensor([lookup.index(i) for i in labels])\n",
    "        elif mode == \"coarse\":\n",
    "            labels = torch.tensor([fine_to_coarse[int(i)] for i in labels])\n",
    "        else:\n",
    "            labels = torch.tensor(labels)\n",
    "        imageGPU = images.to(device)\n",
    "        \n",
    "        outputs = torch.Tensor.cpu(model(imageGPU))\n",
    "        loss = loss_fn(outputs, labels.type(torch.long))\n",
    "        val_running_loss += loss.item()\n",
    "        predicts.append(outputs.detach().numpy())\n",
    "        targets.append(labels)\n",
    "    val_loss = val_running_loss / counter  \n",
    "    predicts = np.vstack(predicts)\n",
    "    targets = np.hstack(targets)\n",
    "    if return_probab:\n",
    "        return predicts, targets\n",
    "    else:\n",
    "        predicts = np.argmax(predicts, axis=1)\n",
    "        return val_loss, accuracy_score(targets, predicts)\n",
    "\n",
    "    \n",
    "\n",
    "def train_and_validate(epochs, model, loss_fn, optimizer, train_dl, test_dl, val_dl,\n",
    "                       mode=\"flat\", fine_to_coarse={}, coarse_label=None, lookup=[], \n",
    "                       model_path = None):\n",
    "\n",
    "    early_stopping=EarlyStopping()\n",
    "    #lr_scheduler = LRScheduler(optimizer)\n",
    "    \n",
    "    if model_path is None:\n",
    "        model_path = '/models/%s.pth' %model.name\n",
    "    min_loss= 1000\n",
    "    max_acc = 0\n",
    "    train_loss= []\n",
    "    val_loss, val_accuracy = [], []\n",
    "    for t in range(epochs):\n",
    "        if mode == \"fine\":\n",
    "            print(f\"Coarse label {coarse_label} Epoch {t+1}\\n-------------------------------\")\n",
    "        else:\n",
    "            print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "        train_epoch_loss = train_loop(train_dl, model, loss_fn, optimizer, mode=mode, fine_to_coarse=fine_to_coarse, lookup=lookup)\n",
    "        val_epoch_loss, val_epoch_acc = eval_model(val_dl, model,loss_fn, mode=mode, fine_to_coarse=fine_to_coarse, lookup=lookup)\n",
    "        \n",
    "        train_loss.append(train_epoch_loss)\n",
    "        val_loss.append(val_epoch_loss)\n",
    "        val_accuracy.append(val_epoch_acc)\n",
    "        if val_epoch_loss < min_loss:\n",
    "            max_acc = val_epoch_acc\n",
    "            min_loss = val_epoch_loss\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "        elif val_epoch_acc>max_acc:\n",
    "            max_acc = val_epoch_acc\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "        early_stopping(val_epoch_loss)\n",
    "        if early_stopping.early_stop:\n",
    "            break\n",
    "        print(\"train loss %s, validation loss %s, validation accuracy %s\" %(train_epoch_loss, \n",
    "                                                                            val_epoch_loss, \n",
    "                                                                            val_epoch_acc))\n",
    "    test_loss, test_acc = eval_model(test_dl, model, loss_fn, mode=mode, fine_to_coarse=fine_to_coarse, lookup=lookup)\n",
    "    print(\"Done! Saved model with test accuracy %s and loss %s\" %(test_acc, test_loss))\n",
    "    # accuracy plots\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.plot(val_accuracy, color='blue', label='validation accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"models/validationAccuracy{model.name}.png\")\n",
    "    plt.show()\n",
    "    # loss plots\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.plot(train_loss, color='orange', label='train loss')\n",
    "    plt.plot(val_loss, color='red', label='validation loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"models/losses{model.name}.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    return  test_acc\n",
    "\n",
    "\n",
    "class EarlyStopping():\n",
    "    \"\"\"\n",
    "    Early stopping to stop the training when the loss does not improve after\n",
    "    certain epochs.\n",
    "    \"\"\"\n",
    "    def __init__(self, patience=5, min_delta=0):\n",
    "        \"\"\"\n",
    "        :param patience: how many epochs to wait before stopping when loss is\n",
    "               not improving\n",
    "        :param min_delta: minimum difference between new loss and old loss for\n",
    "               new loss to be considered as an improvement\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss == None:\n",
    "            self.best_loss = val_loss\n",
    "        elif self.best_loss - val_loss > self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "        elif self.best_loss - val_loss < self.min_delta:\n",
    "            self.counter += 1\n",
    "            print(f\"INFO: Early stopping counter {self.counter} of {self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                print('INFO: Early stopping')\n",
    "                self.early_stop = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5422350",
   "metadata": {},
   "outputs": [],
   "source": [
    "#learning rate is the same for all models\n",
    "learning_rate = 1e-4 \n",
    "teamdrive_root = '../../../teamdrive/transmediasp/kate/'\n",
    "fine_tuned_model_path = teamdrive_root + 'icons_experiment/icons_fine_tuned_model.pth'\n",
    "\n",
    "volumes = [15, 25, 35, 55, 85]\n",
    "seeds = [1234, 6512, 3845, 4321, 5888, 7356, 1834, 4628, 9375, 8372]\n",
    "\n",
    "#create a dictionary to save results in\n",
    "flat_accuracies = defaultdict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95bd669",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dataframe with labels\n",
    "df = pd.read_csv(teamdrive_root + 'icons_experiment/data_frame_with_volume_splits.csv', index_col=0)\n",
    "\n",
    "n_classes = len(df['class'].unique())\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592b7e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()\n",
    "\n",
    "if torch.cuda.is_available():  \n",
    "    dev = \"cuda:0\" \n",
    "else:  \n",
    "    dev = \"cpu\"  \n",
    "    \n",
    "device = torch.device(dev)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5739f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for run, seed in enumerate(seeds):\n",
    "    #set manual seed\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    flat_accuracies['%s' %run] = defaultdict()\n",
    "\n",
    "    for volume in volumes:\n",
    "        print(run, volume)\n",
    "        #prepare data\n",
    "        train_df = df[df['%s_%s' %(volume, run)] == 'train']\n",
    "        test_df = df[df['%s_%s' %(volume, run)] == 'test']\n",
    "        valid_df = df[df['%s_%s' %(volume, run)] == 'valid']\n",
    "\n",
    "        # prepare dataloaders\n",
    "        batch_size = 64\n",
    "\n",
    "        test_dataset = IconDataset(test_df, test_transform, \"\")\n",
    "        train_dataset = IconDataset(train_df, train_transform, \"\")\n",
    "        validation_dataset = IconDataset(valid_df, test_transform, \"\")\n",
    "\n",
    "        dl_train = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        dl_test = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "        dl_valid = torch.utils.data.DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        # train flat model \n",
    "        gc.collect() \n",
    "        torch.cuda.empty_cache()\n",
    "        epochs= 40\n",
    "\n",
    "        model = Resnext50(n_classes, name=\"flat_model_Resnext50_icons_volume_%s_%s\" %(volume, run))\n",
    "        model.state_dict(torch.load(fine_tuned_model_path))               \n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        model.to(device)\n",
    "        loss_fn = torch.nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "        test_accuracy =train_and_validate(epochs, model, loss_fn, optimizer, \n",
    "                                                            dl_train, dl_test, dl_valid,\n",
    "                                                            model_path = teamdrive_root + '/models/base/%s.pth' %model.name )\n",
    "        flat_accuracies['%s' %run]['%s' %volume] = test_accuracy\n",
    "        print(flat_accuracies)\n",
    "        with open(teamdrive_root + '/icons_experiment/flat_accuracies_volume_experiment_2.p', 'wb') as handle:\n",
    "            pickle.dump(flat_accuracies, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fbdacd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
