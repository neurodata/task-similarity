{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tasksim import task_similarity, generate_hierarchy\n",
    "\n",
    "import torchvision\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import pickle\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, file, train=True, classes=[]):\n",
    "        if train:\n",
    "            self.data = pickle.load(open('cifar_resnet50_embed.p', 'rb'))[0]\n",
    "            self.targets = np.concatenate(pickle.load(open('cifar_resnet50_embed.p', 'rb'))[1])\n",
    "        else:\n",
    "            self.data = pickle.load(open('cifar_resnet50_embed.p', 'rb'))[2]\n",
    "            self.targets = np.concatenate(pickle.load(open('cifar_resnet50_embed.p', 'rb'))[3])\n",
    "        \n",
    "        self.classes = classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "cif100 = torchvision.datasets.CIFAR100(root='./data', train=True, download=True)\n",
    "trainset = Dataset('cifar_resnet50_embed.p', classes=cif100.classes)\n",
    "testset = Dataset('cifar_resnet50_embed.p', train=False, classes=cif100.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dimension=1000\n",
    "\n",
    "if data_dimension < trainset.data.shape[1]:\n",
    "    pca = PCA(n_components=data_dimension)\n",
    "    pca.fit(trainset.data)\n",
    "    trainset.data = pca.transform(trainset.data)\n",
    "    testset.data = pca.transform(testset.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_clusters(f, truth, preds, calculate_random=False, n_mc=500, acorn=None):\n",
    "    eval_pred = f(truth, preds)\n",
    "    \n",
    "    if not calculate_random:\n",
    "        return eval_pred\n",
    "    \n",
    "    eval_random = np.zeros(n_mc)\n",
    "    for i in range(n_mc):\n",
    "        np.random.shuffle(preds)\n",
    "        eval_random[i] = f(truth, preds)\n",
    "        \n",
    "    return eval_pred, np.mean(eval_random)\n",
    "\n",
    "def evaluate_accuracy(data, labels, truth, preds, n_trees_coarse=25, n_trees_fine=10, train_flat=True,\n",
    "                     test_data=None, test_labels=None, max_depth=5,\n",
    "                     acorn=None):\n",
    "    classes = np.unique(labels)\n",
    "    \n",
    "    forests_dict = {\n",
    "            'coarse_truth': None, \n",
    "            'fine_truth': {c: None for c in np.unique(truth)},\n",
    "            'coarse_preds': None,\n",
    "            'fine_preds': {c: None for c in np.unique(preds)}, \n",
    "            'flat': None\n",
    "    }\n",
    "    \n",
    "    # Coarse forest\n",
    "    coarse_forest_truth = l2f(default_n_estimators=n_trees_coarse,\n",
    "                        default_max_depth=max_depth)\n",
    "    \n",
    "    coarse_forest_truth.add_task(data, truth[labels])\n",
    "    forests_dict['coarse_truth'] = coarse_forest_truth\n",
    "    \n",
    "    coarse_forest_preds = l2f(default_n_estimators=n_trees_coarse,\n",
    "                        default_max_depth=max_depth)\n",
    "    \n",
    "    coarse_forest_preds.add_task(data, preds[labels])\n",
    "    forests_dict['coarse_preds'] = coarse_forest_preds\n",
    "    \n",
    "    \n",
    "    # Flat forest\n",
    "    n_trees_flat = n_trees_coarse + len(np.unique(truth))*n_trees_fine\n",
    "    \n",
    "    if train_flat:\n",
    "        flat_forest_truth = l2f(default_n_estimators=n_trees_flat,\n",
    "                            default_max_depth=max_depth)\n",
    "        flat_forest_truth.add_task(data, labels)\n",
    "        forests_dict['flat'] = flat_forest_truth\n",
    "        \n",
    "    # Fine forest\n",
    "    for j, parent_class in enumerate(np.unique(truth)):\n",
    "        temp_fine_indices = np.where(truth[labels] == parent_class)[0]\n",
    "        \n",
    "        \n",
    "        fine_forest_truth = l2f(default_n_estimators=n_trees_fine, \n",
    "                               default_max_depth=max_depth\n",
    "                              )\n",
    "        fine_forest_truth.add_task(data[temp_fine_indices], labels[temp_fine_indices])\n",
    "        forests_dict['fine_truth'][j] = fine_forest_truth\n",
    "        \n",
    "    for j, parent_class in enumerate(np.unique(preds)):\n",
    "        temp_fine_indices = np.where(preds[labels] == parent_class)[0]\n",
    "        \n",
    "        fine_forest_preds = l2f(default_n_estimators=n_trees_fine, \n",
    "                               default_max_depth=max_depth\n",
    "                              )\n",
    "        fine_forest_preds.add_task(data[temp_fine_indices], labels[temp_fine_indices])\n",
    "        forests_dict['fine_preds'][j] = fine_forest_preds\n",
    "        \n",
    "        \n",
    "    # Now, calculate accuracies\n",
    "    accuracies = np.zeros(3)\n",
    "    \n",
    "    if test_data is None:\n",
    "        raise ValueError\n",
    "        \n",
    "    n_test, d_test = test_data.shape\n",
    "                \n",
    "    hierarchical_posteriors_truth = np.zeros((n_test, len(classes)))\n",
    "    hierarchical_posteriors_preds = np.zeros((n_test, len(classes)))\n",
    "    \n",
    "    coarse_posteriors_truth = forests_dict['coarse_truth'].predict_proba(test_data, 0)\n",
    "    coarse_posteriors_preds = forests_dict['coarse_preds'].predict_proba(test_data, 0)\n",
    "        \n",
    "    # Hierarchical posteriors & prediction\n",
    "    for j, parent_class in enumerate(np.unique(truth)):\n",
    "        temp_fine_label_indices = np.where(truth == parent_class)[0]\n",
    "        \n",
    "        temp_fine_posteriors = forests_dict['fine_truth'][j].predict_proba(test_data, 0)\n",
    "        hierarchical_posteriors_truth[:, temp_fine_label_indices] = np.multiply(coarse_posteriors_truth[:, j],\n",
    "                                                                     temp_fine_posteriors.T\n",
    "                                                                    ).T\n",
    "        \n",
    "    for j, parent_class in enumerate(np.unique(preds)):\n",
    "        temp_fine_label_indices = np.where(preds == parent_class)[0]\n",
    "\n",
    "        \n",
    "        temp_fine_posteriors = forests_dict['fine_preds'][j].predict_proba(test_data, 0)\n",
    "        hierarchical_posteriors_preds[:, temp_fine_label_indices] = np.multiply(coarse_posteriors_preds[:, j],\n",
    "                                                                     temp_fine_posteriors.T\n",
    "                                                                    ).T\n",
    "        \n",
    "    yhat_hc = np.argmax(hierarchical_posteriors_truth, axis=1)\n",
    "    accuracies[0] = np.mean(yhat_hc == np.array(test_labels))\n",
    "    \n",
    "    yhat_hc = np.argmax(hierarchical_posteriors_preds, axis=1)\n",
    "    accuracies[1] = np.mean(yhat_hc == np.array(test_labels))\n",
    "    \n",
    "    \n",
    "    # Flat posteriors & prediction\n",
    "    if train_flat:\n",
    "        flat_posteriors = forests_dict['flat'].predict_proba(test_data, 0)\n",
    "        yhat_flat = np.argmax(flat_posteriors, axis=1)\n",
    "        accuracies[2] = np.mean(yhat_flat == np.array(test_labels))\n",
    "    \n",
    "    return accuracies[:, np.newaxis].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_results(X, y, X_test, y_test,\n",
    "                    acc_kwargs,\n",
    "                    eval_kwargs=None):\n",
    "    \n",
    "    acc_kwargs['test_data'] = X_test\n",
    "    acc_kwargs['test_labels'] = y_test\n",
    "    \n",
    "    del X_test\n",
    "    del y_test\n",
    "    \n",
    "    accs = evaluate_accuracy(X, y, **acc_kwargs)\n",
    "    \n",
    "    if eval_kwargs is not None:\n",
    "        evals = evaluate_clusters(**eval_kwargs)\n",
    "        return np.array(evals)[:, np.newaxis].T, accs\n",
    "    \n",
    "    return accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "coarse_to_fine_map = {\n",
    "'aquatic_mammals': ['beaver', 'dolphin', 'otter', 'seal', 'whale'],\n",
    "'fish': ['aquarium_fish', 'flatfish', 'ray', 'shark', 'trout'],\n",
    "'flowers': ['orchid', 'poppy', 'rose', 'sunflower', 'tulip'],\n",
    "'food_containers': ['bottle', 'bowl', 'can', 'cup', 'plate'],\n",
    "'fruit_and_vegetables': ['apple', 'mushroom', 'orange', 'pear', 'sweet_pepper'],\n",
    "'household_electrical_devices': ['clock', 'keyboard', 'lamp', 'telephone', 'television'],\n",
    "'household_furniture': ['bed', 'chair', 'couch', 'table', 'wardrobe'],\n",
    "'insects': ['bee', 'beetle', 'butterfly', 'caterpillar', 'cockroach'],\n",
    "'large_carnivores': ['bear', 'leopard', 'lion', 'tiger', 'wolf'],\n",
    "'large_man-made_outdoor_things': ['bridge', 'castle', 'house', 'road', 'skyscraper'],\n",
    "'large_natural_outdoor_scenes': ['cloud', 'forest', 'mountain', 'plain', 'sea'],\n",
    "'large_omnivores_and_herbivores': ['camel', 'cattle', 'chimpanzee', 'elephant', 'kangaroo'],\n",
    "'medium-sized_mammals': ['fox', 'porcupine', 'possum', 'raccoon', 'skunk'],\n",
    "'non-insect_invertebrates': ['crab', 'lobster', 'snail', 'spider', 'worm'],\n",
    "'people': ['baby', 'boy', 'girl', 'man', 'woman'],\n",
    "'reptiles': ['crocodile', 'dinosaur', 'lizard', 'snake', 'turtle'],\n",
    "'small mammals': ['hamster', 'mouse', 'rabbit', 'shrew', 'squirrel'],\n",
    "'trees': ['maple_tree', 'oak_tree', 'palm_tree', 'pine_tree', 'willow_tree'],\n",
    "'vehicles_1': ['bicycle', 'bus', 'motorcycle', 'pickup_truck', 'train'],\n",
    "'vehicles_2': ['lawn_mower', 'rocket', 'streetcar', 'tank', 'tractor']\n",
    "}\n",
    "\n",
    "coarse_number_to_coarse_name = {i: name for i, name in enumerate(coarse_to_fine_map)}\n",
    "\n",
    "def fine_to_coarse(coarse_to_fine):\n",
    "    fine_to_coarse_map = {}\n",
    "    for key in coarse_to_fine:\n",
    "        fines = coarse_to_fine[key]\n",
    "        for f in fines:\n",
    "            fine_to_coarse_map[f] = key\n",
    "            \n",
    "    return fine_to_coarse_map\n",
    "\n",
    "fine_to_coarse_map = fine_to_coarse(coarse_to_fine_map)\n",
    "\n",
    "fine_number_to_fine_name = {i: name for i, name in enumerate(trainset.classes)}\n",
    "fine_name_to_fine_number = {name: i for i, name in fine_number_to_fine_name.items()}\n",
    "\n",
    "for i in range(100):\n",
    "    fine_to_coarse_map[fine_number_to_fine_name[i]]\n",
    "    \n",
    "coarse_name_to_coarse_number = {name: i for i, name in enumerate(coarse_to_fine_map)}\n",
    "\n",
    "coarse_targets = np.array([coarse_name_to_coarse_number[fine_to_coarse_map[fine_number_to_fine_name[y]]] for y in trainset.targets])\n",
    "idx_by_coarse = np.array([np.where(coarse_targets == y)[0] for y in range(20)])\n",
    "idx_by_fine = np.array([np.where(trainset.targets == y)[0] for y in range(100)])\n",
    "\n",
    "\n",
    "test_coarse_targets = np.array([coarse_name_to_coarse_number[fine_to_coarse_map[fine_number_to_fine_name[y]]] for y in testset.targets])\n",
    "test_idx_by_coarse = np.array([np.where(test_coarse_targets == y)[0] for y in range(20)])\n",
    "\n",
    "\n",
    "coarse_names = np.array(list(coarse_name_to_coarse_number.keys()))\n",
    "\n",
    "fine_number_to_coarse_number = {fn: coarse_name_to_coarse_number[\n",
    "                                        fine_to_coarse_map[\n",
    "                                            fine_number_to_fine_name[fn]\n",
    "                                        ]\n",
    "                                    ] for fn in range(100)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2)\n",
    "\n",
    "n_props = [0.2, 0.4, 0.6, 0.8]\n",
    "\n",
    "generate_dist_matrix_kwargs = {'metric':'tasksim', \n",
    "                               'metric_kwargs':{'n_neg_classes': 5, \n",
    "                                                'task_similarity_kwargs': {'transformer_kwargsx': \n",
    "                                                                               {'max_depth':5},\n",
    "                                                                          'transformer_kwargsz':\n",
    "                                                                              {'max_depth':5}}}, \n",
    "                               'function_tuples':None, \n",
    "                               'n_cores':30, \n",
    "                               'acorn':None\n",
    "                              }\n",
    "\n",
    "process_dist_matrix_kwargs = {'make_symmetric': True,\n",
    "                              'scale':True,\n",
    "                             'aug_diag':True,\n",
    "                             }\n",
    "\n",
    "embedding=ASE\n",
    "embedding_kwargs={'n_components':16}\n",
    "cluster=GMM\n",
    "cluster_kwargs = {'selection_criteria': 'aic'}\n",
    "\n",
    "cluster_dists_kwargs = {'embedding':embedding, \n",
    "                        'embedding_kwargs':embedding_kwargs, \n",
    "                        'cluster':cluster, \n",
    "                        'cluster_kwargs':cluster_kwargs\n",
    "                       }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NB: Setting n_mc = 2 and n_props = [0.2, 0.4, 0.6, 0.8] takes 9052 seconds = 2.5 hours to generate all the clusters\n",
    "\n",
    "np.random.seed(1)\n",
    "cluster_tuples = []\n",
    "n_mc = 10\n",
    "n_cores=30\n",
    "\n",
    "for i, prop in enumerate(n_props):\n",
    "    for j in range(n_mc):\n",
    "        inds = stratified_sample(idx_by_fine, p=prop)\n",
    "        cluster_tuples.append((inds,\n",
    "                                  generate_dist_matrix_kwargs, \n",
    "                                  process_dist_matrix_kwargs, \n",
    "                                  cluster_dists_kwargs, \n",
    "                                  max([1, int(n_cores / n_mc)])\n",
    "                              ))\n",
    "\n",
    "\n",
    "use_stored_clusters = False\n",
    "\n",
    "if use_stored_clusters:\n",
    "    clusters = pickle.load(open('newest_clusters.pkl', 'rb'))\n",
    "else:\n",
    "    condensed_func_clusters = lambda x: generate_hierarchy(trainset.data[x[0]], trainset.targets[x[0]],\n",
    "                                                          *x[1:])\n",
    "    start_time = time.time()\n",
    "    clusters = Parallel(n_jobs=1)(delayed(condensed_func_clusters)(tuple_) for tuple_ in cluster_tuples)\n",
    "    clocked = time.time() - start_time\n",
    "    print(clocked)\n",
    "    pickle.dump(clusters, open('newest_clusters.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "f=NMI\n",
    "truth = np.array(list(fine_number_to_coarse_number.values()))\n",
    "calculate_random=True\n",
    "random_nmc=500\n",
    "eval_kwargs = (f, truth, calculate_random, random_nmc)\n",
    "\n",
    "n_trees_coarse=500\n",
    "n_trees_fine=100\n",
    "train_flat=False\n",
    "max_depth=5\n",
    "\n",
    "acc_kwargs = (n_trees_coarse, n_trees_fine, train_flat, testset.data, testset.targets, max_depth)\n",
    "\n",
    "results_tuples = []\n",
    "for i, cluster in enumerate(clusters):\n",
    "    inds = cluster_tuples[i][0]\n",
    "    eval_kwargs = {'f': f,\n",
    "                   'truth': truth,\n",
    "                   'preds': cluster,\n",
    "                   'calculate_random': True,\n",
    "                   'n_mc': random_nmc}\n",
    "    acc_kwargs = {'truth':truth, \n",
    "                  'preds': cluster,\n",
    "                  'n_trees_coarse':n_trees_coarse, \n",
    "                  'n_trees_fine':n_trees_fine, \n",
    "                  'train_flat':train_flat,\n",
    "                  'max_depth':max_depth,\n",
    "                }\n",
    "    results_tuples.append((inds, acc_kwargs, eval_kwargs))\n",
    "    \n",
    "condensed_func_results = lambda x: generate_results(trainset.data[x[0]], trainset.targets[x[0]],\n",
    "                                                   testset.data, np.array(testset.targets),\n",
    "                                                   x[1], \n",
    "                                                   x[2])\n",
    "    \n",
    "results = Parallel(n_jobs=n_cores)(delayed(condensed_func_results)(tuple_) for tuple_ in results_tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_per_prop = int(len(results) / len(n_props))\n",
    "\n",
    "evals = np.zeros((len(n_props), 2, n_per_prop))\n",
    "accs = np.zeros((len(n_props), 3, n_per_prop))\n",
    "\n",
    "for _, list_ in enumerate(results):\n",
    "    n_mc_idx = _ % n_per_prop\n",
    "    prop_idx = int(np.floor((_ / n_per_prop)))\n",
    "    evals[prop_idx,:,n_mc_idx]=list_[0]\n",
    "    accs[prop_idx,:,n_mc_idx]=list_[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-f95c641cf2ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcolors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolor_palette\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Set1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_colors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmean_accs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1,2, figsize=(10,5))\n",
    "\n",
    "colors = sns.color_palette(\"Set1\", n_colors=3)\n",
    "\n",
    "mean_accs = np.mean(accs, axis=-1)\n",
    "mean_evals = np.mean(evals, axis=-1)\n",
    "\n",
    "ax[0].plot(n_props, mean_evals[:,0], label='induced', c=colors[0])\n",
    "ax[0].plot(n_props, mean_evals[:,1], label='random', c=colors[1])\n",
    "\n",
    "ax[0].legend(loc='center left', fontsize=16)\n",
    "\n",
    "ax[0].set_xticks(n_props)\n",
    "# ax[1].set_yticks([0.32, 0.34, 0.36, 0.38])\n",
    "# ax[1].set_ylim(0.31, 0.381)\n",
    "# ax[1].set_ylabel('Accuracy')\n",
    "ax[0].set_xlabel('Proportion of training data', fontsize=18)\n",
    "ax[0].set_ylabel('NMI', fontsize=18)\n",
    "ax[0].tick_params(labelsize=16)\n",
    "# ax[0].set_yticks([0.12, 0.16, 0.20, 0.24])\n",
    "\n",
    "algs = ['truth', 'induced', 'flat']\n",
    "\n",
    "colors = sns.color_palette('Set1', n_colors=3)\n",
    "\n",
    "ax[1].plot(n_props, mean_accs[:,1], label='induced', c=colors[0])\n",
    "ax[1].plot(n_props, mean_accs[:,0], label='truth', c=colors[2])\n",
    "\n",
    "ax[1].plot(n_props, mean_accs[:,2], label='flat', c=colors[1])\n",
    "ax[1].legend(loc='center left', fontsize=16)\n",
    "\n",
    "ax[1].set_xticks(n_props)\n",
    "# ax[1].set_yticks([0.32, 0.34, 0.36, 0.38])\n",
    "# ax[1].set_ylim(0.31, 0.381)\n",
    "ax[1].set_ylabel('Accuracy', fontsize=18)\n",
    "ax[1].set_xlabel('Proportion of training data', fontsize=18)\n",
    "ax[1].tick_params(labelsize=16)\n",
    "\n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hc",
   "language": "python",
   "name": "hc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
