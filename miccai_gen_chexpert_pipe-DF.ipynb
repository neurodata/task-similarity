{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "import torchvision\n",
    "import torch\n",
    "import os\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, plot_roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from graspologic.cluster import GaussianCluster as GMM\n",
    "from collections import defaultdict\n",
    "from proglearn.forest import UncertaintyForest\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kate's script to get auc/95\n",
    "%run -i evaluate.py \n",
    "acorn = 1234\n",
    "torch.manual_seed(acorn)\n",
    "np.random.seed(acorn)\n",
    "\n",
    "torch.cuda.is_available()\n",
    "\n",
    "if torch.cuda.is_available():  \n",
    "    dev = \"cuda:0\" \n",
    "else:  \n",
    "    dev = \"cpu\"  \n",
    "    \n",
    "device = torch.device(dev)  \n",
    "\n",
    "n_iter = 10\n",
    "seeds = np.random.randint(10000, size=n_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#process data, filter out only frontal, ap, fillter out uncertainty in classes we care and fill in rest data\n",
    "def process_data(df):\n",
    "    \n",
    "    print('starting size %s' %len(df))\n",
    "    data = df\n",
    "    #only use frontal/AP data\n",
    "    data = data.loc[data['Frontal/Lateral'] == 'Frontal']\n",
    "    data = data.loc[data['AP/PA'] == 'AP']\n",
    "\n",
    "    \n",
    "    category_names = ['Atelectasis', 'Cardiomegaly', 'Consolidation', 'Edema', 'Pleural Effusion']\n",
    "    \n",
    "    #filter out all uncertainty labels in classes we care about\n",
    "    data = data[category_names]\n",
    "    #tread all empty values in these selected cols as 0\n",
    "    data = data.fillna(0)\n",
    "    #filter out -1 (uncertain labels)\n",
    "    data = data.loc[(data.iloc[:, :] !=-1).all(axis=1)]\n",
    "    #row-idx of the data we care to keep\n",
    "    fly_list = data.index\n",
    "    #reselect from orginal of kept rows\n",
    "    data = df.iloc[fly_list]\n",
    "\n",
    "    #select the cols we care about\n",
    "    wanted_cols = [\"Path\", 'No Finding'] + category_names\n",
    "    data = data[wanted_cols]\n",
    "    \n",
    "    #filter out rows with no label values\n",
    "    data['sum']  = data.iloc[:, 1:].sum(axis=1)\n",
    "    fly_list = data.loc[data['sum']>0].index\n",
    "\n",
    "    \n",
    "    data = df[wanted_cols].iloc[fly_list]\n",
    "    # fill all NA and uncertainty as 0     \n",
    "    data = data.fillna(0)\n",
    "    data = data.replace(-1,0)\n",
    "\n",
    "    print(\"final size %s\" %len(data))\n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting size 234\n",
      "final size 132\n",
      "starting size 223414\n",
      "final size 92771\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Path</th>\n",
       "      <th>No Finding</th>\n",
       "      <th>Atelectasis</th>\n",
       "      <th>Cardiomegaly</th>\n",
       "      <th>Consolidation</th>\n",
       "      <th>Edema</th>\n",
       "      <th>Pleural Effusion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CheXpert-v1.0-small/train/patient00001/study1/...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CheXpert-v1.0-small/train/patient00003/study1/...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>CheXpert-v1.0-small/train/patient00006/study1/...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>CheXpert-v1.0-small/train/patient00007/study1/...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>CheXpert-v1.0-small/train/patient00007/study2/...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Path  No Finding  \\\n",
       "0   CheXpert-v1.0-small/train/patient00001/study1/...         1.0   \n",
       "4   CheXpert-v1.0-small/train/patient00003/study1/...         0.0   \n",
       "11  CheXpert-v1.0-small/train/patient00006/study1/...         1.0   \n",
       "12  CheXpert-v1.0-small/train/patient00007/study1/...         0.0   \n",
       "13  CheXpert-v1.0-small/train/patient00007/study2/...         0.0   \n",
       "\n",
       "    Atelectasis  Cardiomegaly  Consolidation  Edema  Pleural Effusion  \n",
       "0           0.0           0.0            0.0    0.0               0.0  \n",
       "4           0.0           0.0            0.0    1.0               0.0  \n",
       "11          0.0           0.0            0.0    0.0               0.0  \n",
       "12          1.0           1.0            0.0    0.0               0.0  \n",
       "13          1.0           0.0            0.0    0.0               0.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#chexpert data\n",
    "data_root = '/home/weiwya/teamdrive_bak/weiwei_temp_data'\n",
    "test_df = pd.read_csv('%s/CheXpert-v1.0-small/valid.csv' %data_root)\n",
    "test_df = process_data(test_df)\n",
    "\n",
    "train_full = pd.read_csv('%s/CheXpert-v1.0-small/train.csv' %data_root)\n",
    "train_full = process_data(train_full)\n",
    "\n",
    "train_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting size 702\n",
      "final size 396\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Path</th>\n",
       "      <th>No Finding</th>\n",
       "      <th>Atelectasis</th>\n",
       "      <th>Cardiomegaly</th>\n",
       "      <th>Consolidation</th>\n",
       "      <th>Edema</th>\n",
       "      <th>Pleural Effusion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CheXphoto-v1.0/valid/synthetic/digital/patient...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CheXphoto-v1.0/valid/synthetic/digital/patient...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CheXphoto-v1.0/valid/synthetic/digital/patient...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CheXphoto-v1.0/valid/synthetic/digital/patient...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>CheXphoto-v1.0/valid/synthetic/digital/patient...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Path  No Finding  Atelectasis  \\\n",
       "0  CheXphoto-v1.0/valid/synthetic/digital/patient...         0.0          0.0   \n",
       "3  CheXphoto-v1.0/valid/synthetic/digital/patient...         0.0          0.0   \n",
       "4  CheXphoto-v1.0/valid/synthetic/digital/patient...         1.0          0.0   \n",
       "5  CheXphoto-v1.0/valid/synthetic/digital/patient...         0.0          1.0   \n",
       "6  CheXphoto-v1.0/valid/synthetic/digital/patient...         0.0          1.0   \n",
       "\n",
       "   Cardiomegaly  Consolidation  Edema  Pleural Effusion  \n",
       "0           1.0            0.0    0.0               0.0  \n",
       "3           0.0            0.0    1.0               0.0  \n",
       "4           0.0            0.0    0.0               0.0  \n",
       "5           0.0            0.0    0.0               1.0  \n",
       "6           1.0            0.0    0.0               0.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#chexphoto\n",
    "data_root_photo = '/home/weiwya/teamdrive_bak/weiwei_temp_data/CheXphoto/'\n",
    "photo_test_df = pd.read_csv('%s/CheXphoto-v1.0/valid.csv' %data_root_photo)\n",
    "photo_test_df = process_data(photo_test_df)\n",
    "\n",
    "photo_test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChestXRayDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, transform, data_root):\n",
    "        #TODO::put something here that perserves aspect ratio\n",
    "        self.class_names = ['No Finding', 'Atelectasis', 'Cardiomegaly', 'Consolidation', 'Edema', 'Pleural Effusion']\n",
    "        self.image_dir = data_root\n",
    "        self.transform = transform\n",
    "        self.total = len(df)\n",
    "        self.image_names = df['Path'].to_list()\n",
    "        self.labels = df[self.class_names].to_numpy()\n",
    "                    \n",
    "    def __len__(self):\n",
    "        return self.total\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_name = self.image_names[idx]\n",
    "        image_path = os.path.join(self.image_dir, image_name)\n",
    "        image = self.transform(Image.open(image_path).convert('RGB'))\n",
    "        label = self.labels[idx]\n",
    "        return image, label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transformation part  \n",
    "# image_size = (320, 320)\n",
    "\n",
    "image_size = (224, 224)\n",
    "resnet_mean = [0.485, 0.456, 0.406]\n",
    "resnet_std = [0.229, 0.224, 0.225]\n",
    "\n",
    "#Creating a Transformation Object\n",
    "train_transform = torchvision.transforms.Compose([\n",
    "    #Converting images to the size that the model expects\n",
    "    torchvision.transforms.Resize(size=image_size),\n",
    "    torchvision.transforms.RandomHorizontalFlip(), #A RandomHorizontalFlip to augment our data\n",
    "    torchvision.transforms.ToTensor(), #Converting to tensor\n",
    "    #Normalizing the data to the data that the ResNet18 was trained on\n",
    "    torchvision.transforms.Normalize(mean = resnet_mean ,\n",
    "                                    std = resnet_std) \n",
    "    \n",
    "])\n",
    "\n",
    "\n",
    "#Creating a Transformation Object\n",
    "test_transform = torchvision.transforms.Compose([\n",
    "    #Converting images to the size that the model expects\n",
    "    torchvision.transforms.Resize(size=image_size),\n",
    "    # We don't do data augmentation in the test/val set    \n",
    "    torchvision.transforms.ToTensor(), #Converting to tensor\n",
    "    torchvision.transforms.Normalize(mean = resnet_mean,\n",
    "                                    std = resnet_std) \n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_fine_labels(features, org_labels, fine_clfs,  n_fine_classes):\n",
    "    n_samples = len(features)\n",
    "    fine_labels = np.zeros( (n_samples, n_fine_classes))\n",
    "    print(n_samples, n_fine_classes)\n",
    "    curr = 0\n",
    "    for idx , clf in enumerate(fine_clfs):\n",
    "        truth = org_labels[:, idx]\n",
    "        for row, v in enumerate(truth):\n",
    "            if v == 1.:\n",
    "                p = clf.predict(features[row].reshape(1,-1)) + curr\n",
    "                fine_labels[row, p] = 1.\n",
    "        \n",
    "        curr +=clf.n_components_              \n",
    "    return fine_labels\n",
    "\n",
    "def make_coarse_labels(fine_labels, coarse_to_fine):\n",
    "    n_samples = len(fine_labels)\n",
    "    n_labels = len(coarse_to_fine)\n",
    "    labels = np.zeros((n_samples, n_labels))\n",
    "    fine_to_coarse = defaultdict(list)\n",
    "    #get all coarse labels for a fine label\n",
    "    for k, v in coarse_to_fine.items():\n",
    "        for vv in v:\n",
    "            fine_to_coarse[vv].append(k)\n",
    "\n",
    "    #map each fine label to a coarse label\n",
    "    for row, f in enumerate(fine_labels):\n",
    "        for col, v in enumerate(f):\n",
    "            if v == 1.:\n",
    "                cc  = fine_to_coarse[col]\n",
    "                for c in cc:\n",
    "                    labels[row, c] = 1.0\n",
    " \n",
    "    return labels\n",
    "    \n",
    "#extract feature from image tensors    \n",
    "def extract_features(feature_extractor, tensors, batch = 72):\n",
    "\n",
    "    curr = 0\n",
    "    total = len(tensors)\n",
    "    res = []\n",
    "    while curr < total:\n",
    "        curr_batch = tensors[curr: curr+batch]\n",
    "        tensor_gpu = curr_batch.to(device)\n",
    "        outputs = feature_extractor(tensor_gpu)        \n",
    "        outputs = torch.Tensor.cpu(outputs)\n",
    "        outputs = outputs.detach().numpy()\n",
    "        n_samples = outputs.shape[0]\n",
    "        n_features = outputs.shape[1]\n",
    "        outputs.resize(n_samples, n_features)\n",
    "        res.append(outputs)\n",
    "        curr+= batch\n",
    "    res = np.vstack(res)\n",
    "    print(res.shape)\n",
    "    return res\n",
    "#take any model, use its penultimate layer as output features\n",
    "def extract_fetures_targets(feature_extractor, dl):    \n",
    "    features = []\n",
    "    targets = []\n",
    "    for val_step, (images, labels) in enumerate(dl):\n",
    "        imagesGPU = images.to(device)      \n",
    "        outputs = feature_extractor(imagesGPU)        \n",
    "        outputs = torch.Tensor.cpu(outputs)\n",
    "        outputs = outputs.detach().numpy()\n",
    "        features.append(outputs)\n",
    "        targets.append(labels)\n",
    "        \n",
    "    features = np.vstack(features)\n",
    "    targets = np.vstack(targets)\n",
    "    dim = features.shape[1]\n",
    "    features= features.reshape(len(dl.dataset), dim)\n",
    "    torch.cuda.empty_cache()\n",
    "    return features, targets\n",
    "\n",
    "\n",
    "#build model using Resnet50 as backbone\n",
    "class Resnet50Base(torch.nn.Module):\n",
    "    def __init__(self, n_classes, name, starter_model=None):\n",
    "        super().__init__()\n",
    "        #no basemodel, used pre-train resnet\n",
    "        if starter_model is None:\n",
    "            resnet = torchvision.models.resnet18(pretrained=True)\n",
    "            resnet.fc = torch.nn.Sequential(\n",
    "                torch.nn.Dropout(p=0.25),\n",
    "                torch.nn.Linear(in_features=512, out_features=n_classes)\n",
    "            )\n",
    "            self.base_model = resnet\n",
    "        else:\n",
    "            base_model = starter_model.base_model\n",
    "            base_model.fc = torch.nn.Sequential(\n",
    "                torch.nn.Dropout(p=0.25),\n",
    "                torch.nn.Linear(in_features=512, out_features=n_classes)\n",
    "            )\n",
    "            self.base_model = base_model\n",
    "            \n",
    "        self.sigm = torch.nn.Sigmoid()\n",
    "        self.name = name\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.sigm(self.base_model(x))\n",
    "    \n",
    "\n",
    "\n",
    "def get_feature_extractor(model):    \n",
    "    tt = model.base_model\n",
    "    modules=list(tt.children())[:-1]\n",
    "    feature_extractor = torch.nn.Sequential(*modules)\n",
    "    for p in feature_extractor.parameters():\n",
    "        p.requires_grad = False\n",
    "    feature_extractor.to(device)\n",
    "    return feature_extractor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_auc_kate(targets, predicts, class_names, alpha= 0.95):\n",
    "    return eval_auc(targets, predicts, class_names, alpha)\n",
    "       \n",
    "def get_prediction(model,  dl):\n",
    "    model.eval()\n",
    "    predicts = []\n",
    "    targets = []\n",
    "    total_loss = 0\n",
    "    class_lookup = dl.dataset.class_names\n",
    "    n_class = len(class_lookup)\n",
    "\n",
    "    for val_step, (images, labels) in enumerate(dl):\n",
    "\n",
    "        imagesGPU, labelsGPU = images.to(device), labels.to(device)        \n",
    "        outputs = model(imagesGPU)\n",
    "        outputs = torch.Tensor.cpu(outputs)\n",
    "        predicts.append(outputs.detach().numpy())\n",
    "        targets.append(labels)\n",
    "\n",
    "    predicts = np.vstack(predicts)\n",
    "    targets = np.vstack(targets)\n",
    "    return predicts, targets\n",
    "\n",
    "def eval_model(model, dl, alpha =0.95, verbose=True):\n",
    "    predicts, targets = get_prediction(model, dl)\n",
    "    aucs = eval_auc(targets, predicts, dl.dataset.class_names, alpha=alpha)\n",
    "    if verbose:\n",
    "        for k, v in aucs.items():\n",
    "            print(k, v)\n",
    "    return aucs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_fine_clf(features, labels, n_cluster_min=3, n_cluster_max=5, return_cond_mean=False):\n",
    "    fine_clfs = []\n",
    "    n_features = labels.shape[1]\n",
    "    curr = 0\n",
    "    fine_to_org ={}\n",
    "    \n",
    "    conditional_means = []\n",
    "    for i in range(n_features):\n",
    "        ll = labels[:, i]\n",
    "        selected_idx = np.where(ll==1.0)[0]\n",
    "        xx = features[selected_idx]\n",
    "           \n",
    "        clf = GMM(min_components=n_cluster_min, max_components=n_cluster_max, reg_covar=1e-3).fit(xx) \n",
    "        pp = clf.predict(xx) + curr\n",
    "        curr += clf.n_components_\n",
    "\n",
    "        unique_y = np.unique(pp)\n",
    "        for y in unique_y:\n",
    "            fine_to_org[y] = i\n",
    "            \n",
    "        fine_clfs.append(clf) \n",
    "        if return_cond_mean:\n",
    "            means = np.array([\n",
    "                np.mean(features[np.where(pp == c)[0]], axis=0) for c in unique_y])\n",
    "            conditional_means.append(means)\n",
    "            \n",
    "    if not return_cond_mean:\n",
    "        return fine_clfs, fine_to_org, curr\n",
    "    else:\n",
    "        conditional_means = np.vstack(conditional_means)\n",
    "        return fine_clfs, fine_to_org, curr, conditional_means\n",
    "\n",
    "    \n",
    "#1st cluster within a label\n",
    "#then cluster means of each cluster to generate coarse label\n",
    "def gen_coarse_fine_clfs(features, labels, n_cluster_min=3, n_cluster_max=5):\n",
    "    \n",
    "    fine_clf, fine_to_org, n_fine_clusters, conditional_means =  gen_fine_clf(features, labels, n_cluster_min, \n",
    "                                                           n_cluster_max, return_cond_mean=True)\n",
    "    coarse_clf  = GMM(min_components= 7, max_components= 12, reg_covar=1e-3, tol=1e-5)\n",
    "    coarse_clf.fit(conditional_means)\n",
    "    total_coarse_labels = coarse_clf.n_components_\n",
    "    \n",
    "    coarse_to_fine = defaultdict(list)\n",
    "    pp = coarse_clf.predict(conditional_means)\n",
    "    \n",
    "    for i, p in enumerate(pp):\n",
    "        coarse_to_fine[p].append(i)\n",
    "    \n",
    "    return fine_clf, fine_to_org, coarse_clf, coarse_to_fine   \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(epochs, train_model, train_loss_fn, train_optimizer, dl_train, dl_valid, eval_fn):\n",
    "    best_auc_dic = None\n",
    "\n",
    "    for e in range(epochs):\n",
    "        print(e)\n",
    "        train_loss = 0.        \n",
    "        train_model.train() \n",
    "        with tqdm(dl_train, unit=\"batch\") as tepoch:\n",
    "            for images, labels in tepoch:\n",
    "                images, targets = images.to(device), labels.to(device)\n",
    "                train_optimizer.zero_grad()\n",
    "                outputs = train_model(images)\n",
    "                loss = train_loss_fn(outputs, targets.type(torch.float))\n",
    "                #Once we get the loss we need to take a gradient step\n",
    "                loss.backward() #Back propogation\n",
    "                train_optimizer.step() #Completes the gradient step by updating all the parameter values(We are using all parameters)\n",
    "                train_loss += loss.item() #Loss is a tensor which can't be added to train_loss so .item() converts it to float                \n",
    "                tepoch.set_postfix(loss=loss.item())\n",
    "        \n",
    "        print('train_loss %s ' %(train_loss / len(dl_train)))\n",
    "        \n",
    "        curr_auc_dic = eval_fn(train_model, dl_valid, verbose=False)\n",
    "        if (best_auc_dic is None)  or  (best_auc_dic['Average']['AUC'] < curr_auc_dic['Average']['AUC']):\n",
    "            best_auc_dic = curr_auc_dic\n",
    "            torch.save(train_model.state_dict(), 'CheXpert_%s_resnet50' %(train_model.name) )\n",
    "            print('curr best %s' %best_auc_dic['Average']['AUC'])\n",
    "\n",
    "\n",
    "    return 'CheXpert_%s_resnet50' %(train_model.name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_fine_labels(features, org_labels, fine_clfs,  n_fine_classes):\n",
    "    n_samples = len(features)\n",
    "    fine_labels = np.zeros( (n_samples, n_fine_classes))\n",
    "    curr = 0\n",
    "    for idx , clf in enumerate(fine_clfs):\n",
    "        truth = org_labels[:, idx]\n",
    "        for row, v in enumerate(truth):\n",
    "            if v == 1.:\n",
    "                p = clf.predict(features[row].reshape(1,-1)) + curr\n",
    "                fine_labels[row, p] = 1.\n",
    "        \n",
    "        curr +=clf.n_components_              \n",
    "    return fine_labels\n",
    "\n",
    "def make_coarse_labels(fine_labels, coarse_to_fine):\n",
    "    n_samples = len(fine_labels)\n",
    "    n_labels = len(coarse_to_fine)\n",
    "    labels = np.zeros((n_samples, n_labels))\n",
    "    fine_to_coarse = defaultdict(list)\n",
    "    #get all coarse labels for a fine label\n",
    "    for k, v in coarse_to_fine.items():\n",
    "        for vv in v:\n",
    "            fine_to_coarse[vv].append(k)\n",
    "\n",
    "    #map each fine label to a coarse label\n",
    "    for row, f in enumerate(fine_labels):\n",
    "        for col, v in enumerate(f):\n",
    "            if v == 1.:\n",
    "                cc  = fine_to_coarse[col]\n",
    "                for c in cc:\n",
    "                    labels[row, c] = 1.0\n",
    " \n",
    "    return labels\n",
    "\n",
    "def get_fine_data_target(features, fine_targets,  wanted_labels):\n",
    "    n_samples = features.shape[0]\n",
    "    dim = len(wanted_labels)\n",
    "    selected_fine_targets = fine_targets[:, wanted_labels]    \n",
    "    #then select the rows where there is contet\n",
    "    kept_features = []\n",
    "    kept_targets = []\n",
    "    for row in range(n_samples):\n",
    "        v = selected_fine_targets[row]\n",
    "        if sum(v) > 0:\n",
    "            kept_features.append(features[row])\n",
    "            kept_targets.append(v)\n",
    "    kept_targets = np.vstack(kept_targets)\n",
    "    kept_features = np.vstack(kept_features)\n",
    "    return kept_features, kept_targets\n",
    "\n",
    "def predict_hierachy(features, coarse_clf, clf_fines, fine_to_org, coarse_to_fine, n_actual_class):\n",
    "    n_samples = len(features)\n",
    "    n_fine_classes = len(fine_to_org)\n",
    "    predict_coarse = coarse_clf.predict(features)\n",
    "    predics_fines= np.zeros((n_samples, n_fine_classes))\n",
    "    \n",
    "    for k, clf in clf_fines.items():\n",
    "        cols = coarse_to_fine[k]\n",
    "        if len(cols) == 1:\n",
    "            c = cols[0]\n",
    "            predics_fines[:, c] += 1\n",
    "        else:\n",
    "            pp = clf.predict(features)\n",
    "            for idx, c in enumerate(cols):\n",
    "                predics_fines[:, c] += pp[:, idx]\n",
    "\n",
    "    predicts = np.zeros((n_samples, n_actual_classes ))\n",
    "    fine_to_coarse = defaultdict(list)\n",
    "\n",
    "    for k, v in coarse_to_fine.items():\n",
    "        for vv in v:\n",
    "            fine_to_coarse[vv].append(k)\n",
    "\n",
    "    counts = defaultdict(int)\n",
    "    for fine, org in fine_to_org.items():\n",
    "        cc = fine_to_coarse[fine]\n",
    "        p_fine = predics_fines[:, fine]\n",
    "        for c in cc:\n",
    "            p_coarse = predict_coarse[:, c]\n",
    "            predicts[:, org] += p_coarse*p_fine\n",
    "            counts[org] += 1\n",
    "    \n",
    "    for i in range(n_actual_class):\n",
    "        if counts[i] != 0:\n",
    "            predicts[:, i] /=counts[i]\n",
    "    return predicts\n",
    "\n",
    "#getto bagging DF, not configured posterior \n",
    "class SimpleDF():\n",
    "    def __init__(self, n_estimators, max_depth, sample_size = 0.75):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.sample_size = sample_size\n",
    "        self.trees = None\n",
    "        self.dim = 0\n",
    "    \n",
    "    def fit(self, X, y, verbose = False):\n",
    "        self.trees = []\n",
    "        self.dim = y.shape[1]\n",
    "        for i in range(self.n_estimators):\n",
    "            if self.sample_size < 1:\n",
    "                cX, _, cY, _ = train_test_split(X, y, train_size=self.sample_size)\n",
    "            else:\n",
    "                cX = X \n",
    "                cY = y\n",
    "            df = DecisionTreeClassifier(max_depth=self.max_depth)\n",
    "            df.fit(cX,cY)\n",
    "            self.trees.append(df)\n",
    "            if verbose:\n",
    "                print('build tree %s' %i+1)\n",
    "        \n",
    "    def predict(self, X, verbose=False):\n",
    "        n_samples = X.shape[0]\n",
    "        res = np.zeros((n_samples, self.dim))\n",
    "        for i in range(self.n_estimators):\n",
    "            p = self.trees[i].predict(X)\n",
    "            res += p\n",
    "            if verbose:\n",
    "                print('predicted %s' %i+1)\n",
    "        return res /self.n_estimators\n",
    "    \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "acorn = 1234\n",
    "torch.manual_seed(acorn)\n",
    "np.random.seed(acorn)\n",
    "\n",
    "\n",
    "seeds = np.random.randint(10000, size=1000)\n",
    "batch_size = 16\n",
    "train_size = 0.01\n",
    "\n",
    "#generate n_iter times of train/validate split\n",
    "trains, validates = [],[]\n",
    "for i in range(100):\n",
    "    train, validate = train_test_split(train_full, test_size=1-train_size, random_state=seeds[i], shuffle=True)\n",
    "    trains.append(train)\n",
    "    validates.append(validate)\n",
    "        \n",
    "\n",
    "#acutal test df for chexperd\n",
    "test_dataset = ChestXRayDataset(test_df, test_transform, data_root)\n",
    "dl_test = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "#actual test df for \n",
    "test_photo_dataset = ChestXRayDataset(photo_test_df, test_transform, data_root_photo)\n",
    "dl_test_photo = torch.utils.data.DataLoader(test_photo_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/58 [00:00<00:05,  9.81batch/s, loss=0.653]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 58/58 [00:05<00:00, 11.00batch/s, loss=0.534]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss 0.507447447242408 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 2/58 [00:00<00:04, 11.29batch/s, loss=0.387]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curr best 0.7580720362886509\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 58/58 [00:05<00:00, 11.06batch/s, loss=0.435]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss 0.44875717933835657 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 2/58 [00:00<00:05, 10.96batch/s, loss=0.409]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curr best 0.7743931129394735\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 58/58 [00:05<00:00, 10.36batch/s, loss=0.452]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss 0.423359257907703 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 2/58 [00:00<00:04, 11.71batch/s, loss=0.395]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 58/58 [00:05<00:00, 10.88batch/s, loss=0.428]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss 0.4118287347514054 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 2/58 [00:00<00:05, 11.03batch/s, loss=0.253]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curr best 0.7852306558238806\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 58/58 [00:05<00:00, 11.05batch/s, loss=0.32] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss 0.3841230941229853 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/58 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curr best 0.7944779777998248\n",
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 58/58 [00:06<00:00,  8.85batch/s, loss=0.451]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss 0.35425084829330444 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/58 [00:00<?, ?batch/s, loss=0.315]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curr best 0.7978933893687644\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 58/58 [00:05<00:00, 10.88batch/s, loss=0.278]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss 0.31253676758757953 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 2/58 [00:00<00:04, 11.85batch/s, loss=0.25]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 58/58 [00:05<00:00, 10.47batch/s, loss=0.266]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss 0.2789884949552602 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/58 [00:00<00:08,  7.09batch/s, loss=0.243]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 36/58 [00:05<00:02,  9.57batch/s, loss=0.278]"
     ]
    }
   ],
   "source": [
    "base_test_auc = []\n",
    "hierachy_test_auc = []\n",
    "\n",
    "base_test_auc_photo = []\n",
    "hierachy_test_auc_photo = []\n",
    "\n",
    "debug_auc =[]\n",
    "debug_auc_photo = []\n",
    "\n",
    "debug_hierachy_auc = []\n",
    "debug_hierachy_auc_photo = []\n",
    "\n",
    "n_iter = 20\n",
    "train_epoch= 10\n",
    "\n",
    "for iteration in range(n_iter):\n",
    "    print(iteration)\n",
    "    torch.cuda.empty_cache()\n",
    "    train_df, validate_df = trains[iteration], validates[iteration][:1000]\n",
    "    train_dataset = ChestXRayDataset(train_df, train_transform, data_root)\n",
    "    valid_dataset = ChestXRayDataset(validate_df, test_transform, data_root)\n",
    "\n",
    "    dl_train = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    dl_valid = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Initialize the model\n",
    "    c_model = Resnet50Base(len(train_dataset.class_names), 'base_model_%s_%s' %(train_size, iteration))\n",
    "    c_model.to(device)\n",
    "    c_loss_fn = torch.nn.BCELoss().to(device)\n",
    "    c_optimizer = torch.optim.Adam(c_model.parameters(), lr=5e-4)\n",
    "    \n",
    "    best_model_name = train_model(train_epoch, c_model, c_loss_fn, c_optimizer, dl_train, dl_valid, eval_model)\n",
    "    print('done eval base model ')\n",
    "    \n",
    "    #reload best model for embedding\n",
    "    print('using model %s' %best_model_name)\n",
    "    c_model = Resnet50Base(len(train_dataset.class_names), '')\n",
    "    c_model.load_state_dict(torch.load(best_model_name))\n",
    "    c_model.to(device)\n",
    "\n",
    "    #evalute test sets at per formance\n",
    "    test_auc = eval_model(c_model, dl_test, verbose=False)\n",
    "    test_photo_auc = eval_model(c_model, dl_test_photo, verbose=False)\n",
    "    base_test_auc.append(test_auc)\n",
    "    base_test_auc_photo.append(test_photo_auc)\n",
    "    print('auc at best base model %s %s' %(test_auc['Average']['AUC'], test_photo_auc['Average']['AUC'] ))\n",
    "    debug_auc.append(test_auc['Average']['AUC'])\n",
    "    debug_auc_photo.append(test_photo_auc['Average']['AUC'] )\n",
    "    \n",
    "    \n",
    "    \n",
    "    embd_model = get_feature_extractor(c_model)\n",
    "    embd_model.to(device)\n",
    "    train_features, train_targets = extract_fetures_targets(embd_model, dl_train)\n",
    "    \n",
    "    fine_clfs, fine_to_org, coarse_clf, coarse_to_fine = gen_coarse_fine_clfs(train_features, train_targets, n_cluster_min=1, n_cluster_max=12 )\n",
    "    n_fine_classes = len(fine_to_org)\n",
    "    n_coarse_classes = len(coarse_to_fine)\n",
    "    print('done generating clfs: n_fine_clf: %s n_coarse: %s' %(n_fine_classes, n_coarse_classes))\n",
    "    \n",
    "    validate_features, validate_target = extract_fetures_targets(embd_model, dl_valid)\n",
    "    test_features, test_target = extract_fetures_targets(embd_model, dl_test)\n",
    "    test_photo_features, test_photo_targets = extract_fetures_targets(embd_model, dl_test_photo)\n",
    "    \n",
    "    train_fine_targets = make_fine_labels(train_features, train_targets, fine_clfs, len(fine_to_org))\n",
    "    train_coarse_targets = make_coarse_labels(train_fine_targets, coarse_to_fine)\n",
    "    coarse_clf = SimpleDF(n_estimators=100, sample_size=0.75, max_depth= 20)\n",
    "    coarse_clf.fit(train_features, train_coarse_targets)\n",
    "    clf_fines = {}\n",
    "    for k, v in coarse_to_fine.items():\n",
    "        f, t = get_fine_data_target(train_features, train_fine_targets, v)\n",
    "        clf = SimpleDF(n_estimators=100, sample_size=1.0, max_depth=10)\n",
    "        clf.fit(f,t)\n",
    "        clf_fines[k] = clf\n",
    "    n_actual_classes = train_targets.shape[1]\n",
    "    predicts_test = predict_hierachy(test_features, coarse_clf, clf_fines, fine_to_org, coarse_to_fine, n_actual_classes)\n",
    "    test_auc = eval_auc(test_target, predicts_test, dl_train.dataset.class_names, alpha=0.95)\n",
    "    predicts_photo = predict_hierachy(test_photo_features, coarse_clf, clf_fines, fine_to_org, coarse_to_fine, n_actual_classes)\n",
    "    test_photo_auc = eval_auc(test_photo_targets, predicts_photo, dl_train.dataset.class_names, alpha=0.95)\n",
    "\n",
    "    print('auc at best hierachy model %s %s' %(test_auc['Average']['AUC'], test_photo_auc['Average']['AUC'] ))\n",
    "    debug_hierachy_auc.append(test_auc['Average']['AUC'])\n",
    "    debug_hierachy_auc_photo.append(test_photo_auc['Average']['AUC'])\n",
    "    \n",
    "    hierachy_test_auc.append(test_auc)\n",
    "    hierachy_test_auc_photo.append(test_photo_auc)\n",
    "    \n",
    "    print(np.average(debug_auc), np.average(debug_auc_photo))\n",
    "    print(np.average(debug_hierachy_auc), np.average(debug_hierachy_auc_photo))\n",
    "    print('!!!!!!!!!!!')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.624059829059829 0.6343837712356231"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
